import argparse
import dask
import datetime as dt
import itertools
import logging
import numpy as np
import os
import pandas as pd
import time

from dask import config as cfg
from dask import bag as db
from dask import dataframe as dd
from dask.distributed import Client
from pathlib import Path
from typing import Any, Tuple

from soundade.hpc.arguments import DaskArgumentParser
from soundade.hpc.cluster import clusters
from soundade.audio.feature.scalar import Features
from soundade.data.dataset import Dataset
from soundade.data.bag import (
    create_file_load_dictionary,
    load_audio_from_path,
    log_features,
    transform_features,
    remove_dc_offset,
    apply_high_pass_filter,
    extract_scalar_features_from_audio,
)

PYARROW_VERSION = "2.6"

logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

def acoustic_features_meta():
    return pd.DataFrame({
        "sr": pd.Series(dtype="int32[pyarrow]"),
        "segment_id": pd.Series(dtype="string[pyarrow]"),
        "segment_idx": pd.Series(dtype="string[pyarrow]"),
        "file_id": pd.Series(dtype="string[pyarrow]"),
        "duration": pd.Series(dtype="float64[pyarrow]"),
        "offset": pd.Series(dtype="float64[pyarrow]"),
        "frame_length": pd.Series(dtype="int32[pyarrow]"),
        "hop_length": pd.Series(dtype="int32[pyarrow]"),
        "n_fft": pd.Series(dtype="int32[pyarrow]"),
        "feature_length": pd.Series(dtype="int32[pyarrow]"),
        **{
            f"{feature.name}": pd.Series(dtype="float64[pyarrow]")
            for feature in Features
        },
        "log acoustic evenness index": pd.Series(dtype="float64[pyarrow]"),
        "log root mean square": pd.Series(dtype="float64[pyarrow]"),
        "log(1-temporal entropy)": pd.Series(dtype="float64[pyarrow]"),
    })

def acoustic_features(
    root_dir: Path,
    config_path: Path,
    files_df: pd.DataFrame,
    outfile: Path,
    dc_correction: int,
    high_pass_filter: int,
    npartitions: int = None,
    compute: bool = False,
    **kwargs: Any,
) -> Tuple[dd.DataFrame, dd.Scalar | None] | pd.DataFrame:
    root_dir = Path(root_dir).expanduser()
    dataset = Dataset.from_config_path(config_path)

    log.info("Setting up acoustic feature extraction pipeline.")
    log.info("Corrupt files will be filtered.")

    audio_dicts = files_df[files_df["valid"]].to_dict(orient="records")
    b = db.from_sequence(audio_dicts, npartitions=npartitions)

    log.info(f"Partitions after load: {b.npartitions}")
    log.info(f"Loading audio at {dataset.sample_rate}Hz in segments of duration {dataset.segment_duration}s")

    b = (
        b.map(create_file_load_dictionary, root_dir=root_dir, seconds=dataset.segment_duration, sr=dataset.sample_rate)
        .flatten()
        .map(load_audio_from_path, root_dir=root_dir, sr=dataset.sample_rate)
    )

    if dc_correction:
        log.info("Removing DC offset")
        b = b.map(remove_dc_offset)

    if high_pass_filter:
        log.info(f"Applying highpass filter at {dataset.fcut}")
        b = b.map(apply_high_pass_filter, fcut=dataset.fcut, forder=2, fname="butter", ftype="highpass")

    params = dataset.acoustic_feature_params
    log.info(f"Extracting acoustic features with FFT {params=} for {len(audio_dicts)}")
    epsilon = 1e-8
    ddf = (
        b.map(extract_scalar_features_from_audio, **params)
        .map(log_features, features=["acoustic evenness index", "root mean square"])
        .map(transform_features, lambda f: np.log(1.0 - np.array(f) + epsilon), name="log(1-{f})", features=["temporal entropy"])
        .to_dataframe(meta=acoustic_features_meta())
    )

    log.info("Reshaping features dataframe to long form")

    future: dd.Scalar = ddf.to_parquet(
        Path(outfile),
        version=PYARROW_VERSION,
        allow_truncated_timestamps=True,
        write_index=False,
        compute=False,
    )

    log.info(f"Queued {len(audio_dicts)} files for acoustic feature extraction. Will persist to {outfile}")

    if compute:
        dask.compute(future)
        return pd.read_parquet(Path(outfile)), None

    return ddf, future

def main(
    infile: Path,
    cluster: str | None,
    memory: int,
    cores: int,
    jobs: int,
    queue: str,
    local: bool,
    threads_per_worker: int,
    debug: bool,
    **kwargs: Any,
) -> dd.DataFrame | None:
    """
    Process audio files using the specified parameters.

    Args:
        root_dir: Path to the audio root directory
        infile(str, required): Path to a file index generated by 'index_audio' command.
        outfile (str, required): Output file path.
        cluster (str, optional): Name of the cluster to use. 'arc' or 'altair' or None if local==True. Defaults to None.
        memory (int, optional): Memory limit for each worker in GB. Defaults to 32.
        cores (int, optional): Number of CPU cores per worker. Defaults to 8.
        jobs (int, optional): Number of worker jobs to start. Defaults to 12.
        npartitions (int, optional): Number of partitions for Dask DataFrame. Defaults to 2000.
        local (bool, optional): Flag indicating whether to use a local cluster for computation.
        compute (bool, optional): Flag indicating whether to persist parquet eagerly. Defaults to false.
        debug (bool, optional): Flag indicating whether to run synchronously. Defaults to false.

    Returns:
        dd.DataFrame

    Raises:
        ValueError: If an error occurs during processing.

    Examples:
        >>> main(indir='./data/ecolistening', outfile='./data/processed/ecolistening/features.parquet',
        ...      frame=2048, hop=512, n_fft=1024,
        ...      local=True, compute=True)
        <Client: ...
    """
    if not local:
        cluster = clusters[cluster](
            cores=cores,
            memory=memory,
            queue=queue,
            name=None
        )
        log.info(cluster.job_script())
        cluster.scale(jobs=jobs)
        client = Client(cluster)
    else:
        if debug:
            cfg.set(scheduler='synchronous')

        client = Client(
            n_workers=cores,
            threads_per_worker=threads_per_worker,
            memory_limit=f'{memory}GiB'
        )
        log.info(client)

    start_time = time.time()

    acoustic_features(
        files_df=pd.read_parquet(infile),
        **kwargs,
    )

    log.info(f"Acoustic feature extraction complete")
    log.info(f"Time taken: {str(dt.timedelta(seconds=time.time() - start_time))}")

def get_base_parser():
    parser = argparse.ArgumentParser(
        description='Extract acoustic features from audio files',
        add_help=False,
    )
    parser.add_argument(
        "--root-dir",
        type=lambda p: Path(p).expanduser(),
        help="Root directory of the audio files (nested folder structure permitted)",
    )
    parser.add_argument(
        '--config-path',
        type=lambda p: Path(p).expanduser(),
        help='/path/to/dataset/config.yaml',
    )
    parser.add_argument(
        '--infile',
        type=lambda p: Path(p).expanduser(),
        default=None,
        help='File index parquet file'
    )
    parser.add_argument(
        '--outfile',
        type=lambda p: Path(p).expanduser(),
        default=None,
        help='Parquet file to save results'
    )
    parser.add_argument(
        '--cluster',
        default='artemis',
        help='Which cluster to use?'
    )
    parser.add_argument(
        '--memory',
        default=16,
        type=int,
        help='Amount of memory required in GB (total per node)'
    )
    parser.add_argument(
        '--cores',
        default=1,
        type=int,
        help='Number of cores per node'
    )
    parser.add_argument(
        '--jobs',
        default=4,
        type=int,
        help='Number of simultaneous jobs'
    )
    parser.add_argument(
        '--queue',
        default="general",
        type=str,
        help='SLURM job queue name',
    )
    parser.add_argument(
        '--local',
        type=bool,
        default=True,
        help="When true, run locally, when false, run on the cluster"
    )
    parser.add_argument(
        "--threads-per-worker",
        type=int,
        help="Threads per worker",
    )
    parser.add_argument(
        '--debug',
        default=False,
        action='store_true',
        help='Sets single-threaded for debugging.'
    )
    parser.add_argument(
        "--dc-correction",
        type=int,
        help="Apply DC Correction by subtracting the mean",
    )
    parser.add_argument(
        "--high-pass-filter",
        type=int,
        help="Apply a high pass filter",
    )
    parser.add_argument(
        '--npartitions',
        default=None,
        type=int,
        help='Number of dask partitions for the data'
    )
    parser.add_argument(
        '--compute',
        type=int,
        default=1,
        help='Execute the program immediately'
    )
    parser.set_defaults(func=main, **{
        "root_dir": "/data",
        "config_path": "/config.yml",
        "infile": "/results/files_table.parquet",
        "outfile": "/results/recording_acoustic_features_table.parquet",

        'local': os.environ.get("LOCAL", True),
        "memory": os.environ.get("MEM_PER_CPU", 0),
        "cores": os.environ.get("CORES", 0),
        "threads_per_worker": os.environ.get("THREADS_PER_WORKER", 1),

        "dc_correction": os.environ.get("DC_CORR", 0),
        "high_pass_filter": os.environ.get("HIGH_PASS_FILTER", 1),
    })
    return parser

def register_subparser(subparsers):
    parser = subparsers.add_parser(
        "acoustic_features",
        help="Extract acoustic features",
        parents=[get_base_parser()],
        add_help=True,
    )

if __name__ == '__main__':
    parser = get_base_parser()
    args = parser.parse_args()
    log.info(args)
    args.func(**vars(args))
